# 🌟 Awesome AI-Scientist Benchmarks [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

A curated list of **AI-Scientist Benchmarks** to evaluate **Proposal2Code** pipelines, **Code Agents**, and AI research capabilities.  
The goal: **exploring how AI can autonomously generate research ideas, implement them, and validate results.**

---

## 🎯 Purpose
- To explore benchmarks for **Proposal2Code**.  
- To evaluate and improve **CodeAgent** capabilities.  

---

## 📚 Core AI-Scientist Benchmarks

- **[OpenAI PaperBench](https://arxiv.org/abs/2504.01848)** — End-to-end research paper reproduction.  
- **[OpenAI MLEBench](https://arxiv.org/abs/2504.01848)** — Kaggle competition leaderboard performance.  
- **[Stanford ResearchCodeBench](https://arxiv.org/abs/2506.02314)** — Measuring the implementation of innovative research ideas.  
- **[EXP-Bench: Can AI Conduct AI Research Experiments?](https://arxiv.org/abs/2505.24785)**  
- **[MLR-Bench](https://arxiv.org/abs/2505.19955)** — Open-ended ML research capabilities.  
- **[SciReplicate-Bench](https://arxiv.org/abs/2504.00255)** — Agent-driven algorithmic reproduction.  
- **[ML-Bench](https://arxiv.org/abs/2311.09835)** — Repo-level ML coding evaluation.  
- **[SUPER](https://arxiv.org/abs/2409.07440)** — Research repository setup & execution.  
- **[AAAR-1.0](https://arxiv.org/abs/2410.22394)** — Assessing AI’s potential to assist research.  
- **[ScienceAgentBench](https://arxiv.org/abs/2410.05080)** — Language agents for scientific discovery.  
- **[CORE-Bench](https://arxiv.org/abs/2409.11363)** — Computational reproducibility in published research.  
- **[Auto-Bench](https://arxiv.org/abs/2502.15224)** — Automated benchmark for discovery tasks.  
- **[MLGym](https://arxiv.org/abs/2502.14499)** — A framework for AI research agents.  
- **[SciCode](https://arxiv.org/abs/2407.13168)** — Research coding curated by scientists.  
- **[RE-Bench](https://arxiv.org/abs/2411.15114)** — Comparing AI R&D to human experts.  

---

## 🤖 General Agent Benchmarks

- **[GAIA](https://arxiv.org/abs/2311.12983)** — General AI assistant benchmark.  
- **[SWE-bench](https://arxiv.org/abs/2310.06770)** — Real-world GitHub issue resolution.  
- **[Humanity’s Last Exam](https://arxiv.org/abs/2501.14249)** — Frontier test of general reasoning ability.  

---

## 🧑‍💻 Code Agent Benchmarks

- **[Paper2Code](https://arxiv.org/abs/2504.17192)** — From paper to runnable code.  
- **[AlphaGo Moment for Model Architecture Discovery](https://arxiv.org/abs/2507.18074)** — Large-scale AI research experiments.  

---

## 🧬 AI-Scientist in Specific Domains

- **[Benchmarking AI scientists in omics](https://arxiv.org/abs/2505.08341v1)** — Biological data-driven research.  
- **[SciAssess](https://arxiv.org/abs/2403.01976)** — Literature analysis proficiency.  
- **[BixBench](https://arxiv.org/abs/2503.00096)** — Computational biology agent evaluation.  

---

## 📝 Survey & Meta Benchmarks

- **[ResearchArena](https://arxiv.org/abs/2406.10291)** — Evaluating research agents’ ability to collect & organize information.  

---

## 📊 Data Science Benchmarks

- **[DataSciBench](https://arxiv.org/abs/2502.13897)** — Data science agent benchmark.  
- **[InfiAgent-DABench](https://arxiv.org/abs/2401.05507)** — Data analysis tasks for agents.  
- **[Tapilot-Crossing](https://arxiv.org/abs/2403.05307)** — Interactive data analysis evaluation.  

---

## ⭐ Contributing
Contributions are welcome! Please open an issue or PR to add missing benchmarks.

---

## 📜 License
[MIT](https://opensource.org/licenses/MIT) — Free to use, share, and contribute.

