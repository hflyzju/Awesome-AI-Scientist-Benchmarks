# ğŸŒŸ Awesome AI-Scientist Benchmarks [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

A curated list of **AI-Scientist Benchmarks** to evaluate **Proposal2Code** pipelines, **Code Agents**, and AI research capabilities.  
The goal: **exploring how AI can autonomously generate research ideas, implement them, and validate results.**

---

## ğŸ¯ Purpose
- To explore benchmarks for **Proposal2Code**.  
- To evaluate and improve **CodeAgent** capabilities.  

---

## ğŸ“š Core AI-Scientist Benchmarks

- **[OpenAI PaperBench](https://arxiv.org/abs/2504.01848)** â€” End-to-end research paper reproduction.  
- **[OpenAI MLEBench](https://arxiv.org/abs/2504.01848)** â€” Kaggle competition leaderboard performance.  
- **[Stanford ResearchCodeBench](https://arxiv.org/abs/2506.02314)** â€” Measuring the implementation of innovative research ideas.  
- **[EXP-Bench: Can AI Conduct AI Research Experiments?](https://arxiv.org/abs/2505.24785)**  
- **[MLR-Bench](https://arxiv.org/abs/2505.19955)** â€” Open-ended ML research capabilities.  
- **[SciReplicate-Bench](https://arxiv.org/abs/2504.00255)** â€” Agent-driven algorithmic reproduction.  
- **[ML-Bench](https://arxiv.org/abs/2311.09835)** â€” Repo-level ML coding evaluation.  
- **[SUPER](https://arxiv.org/abs/2409.07440)** â€” Research repository setup & execution.  
- **[AAAR-1.0](https://arxiv.org/abs/2410.22394)** â€” Assessing AIâ€™s potential to assist research.  
- **[ScienceAgentBench](https://arxiv.org/abs/2410.05080)** â€” Language agents for scientific discovery.  
- **[CORE-Bench](https://arxiv.org/abs/2409.11363)** â€” Computational reproducibility in published research.  
- **[Auto-Bench](https://arxiv.org/abs/2502.15224)** â€” Automated benchmark for discovery tasks.  
- **[MLGym](https://arxiv.org/abs/2502.14499)** â€” A framework for AI research agents.  
- **[SciCode](https://arxiv.org/abs/2407.13168)** â€” Research coding curated by scientists.  
- **[RE-Bench](https://arxiv.org/abs/2411.15114)** â€” Comparing AI R&D to human experts.  

---

## ğŸ¤– General Agent Benchmarks

- **[GAIA](https://arxiv.org/abs/2311.12983)** â€” General AI assistant benchmark.  
- **[SWE-bench](https://arxiv.org/abs/2310.06770)** â€” Real-world GitHub issue resolution.  
- **[Humanityâ€™s Last Exam](https://arxiv.org/abs/2501.14249)** â€” Frontier test of general reasoning ability.  

---

## ğŸ§‘â€ğŸ’» Code Agent Benchmarks

- **[Paper2Code](https://arxiv.org/abs/2504.17192)** â€” From paper to runnable code.  
- **[AlphaGo Moment for Model Architecture Discovery](https://arxiv.org/abs/2507.18074)** â€” Large-scale AI research experiments.  

---

## ğŸ§¬ AI-Scientist in Specific Domains

- **[Benchmarking AI scientists in omics](https://arxiv.org/abs/2505.08341v1)** â€” Biological data-driven research.  
- **[SciAssess](https://arxiv.org/abs/2403.01976)** â€” Literature analysis proficiency.  
- **[BixBench](https://arxiv.org/abs/2503.00096)** â€” Computational biology agent evaluation.  

---

## ğŸ“ Survey & Meta Benchmarks

- **[ResearchArena](https://arxiv.org/abs/2406.10291)** â€” Evaluating research agentsâ€™ ability to collect & organize information.  

---

## ğŸ“Š Data Science Benchmarks

- **[DataSciBench](https://arxiv.org/abs/2502.13897)** â€” Data science agent benchmark.  
- **[InfiAgent-DABench](https://arxiv.org/abs/2401.05507)** â€” Data analysis tasks for agents.  
- **[Tapilot-Crossing](https://arxiv.org/abs/2403.05307)** â€” Interactive data analysis evaluation.  

---

## â­ Contributing
Contributions are welcome! Please open an issue or PR to add missing benchmarks.

---

## ğŸ“œ License
[MIT](https://opensource.org/licenses/MIT) â€” Free to use, share, and contribute.

